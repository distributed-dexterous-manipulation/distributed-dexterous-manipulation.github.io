{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from gpt_adaln_core import Transformer\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e72b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_pos_world = np.zeros((8,8,2))\n",
    "kdtree_positions_world = np.zeros((64, 2))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i%2!=0:\n",
    "            finger_pos = np.array((i*0.0375, j*0.043301 - 0.02165))\n",
    "            rb_pos_world[i,j] = np.array((i*0.0375, j*0.043301 - 0.02165))\n",
    "        else:\n",
    "            finger_pos = np.array((i*0.0375, j*0.043301))\n",
    "            rb_pos_world[i,j] = np.array((i*0.0375, j*0.043301))\n",
    "        kdtree_positions_world[i*8 + j, :] = rb_pos_world[i,j]\n",
    "\n",
    "np.min(kdtree_positions_world, axis=0), np.max(kdtree_positions_world, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationDataset(Dataset):\n",
    "    def __init__(self, states, actions, next_states, pos, num_agents, rewards, done):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.next_states = next_states\n",
    "        self.pos = pos\n",
    "        self.num_agents = num_agents\n",
    "        self.rewards = rewards\n",
    "        self.done = done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.states[idx]\n",
    "        action = self.actions[idx]\n",
    "        next_states = self.next_states[idx]\n",
    "        pos = self.pos[idx]\n",
    "        num_agents = self.num_agents[idx]\n",
    "        reward = self.rewards[idx]\n",
    "        done = self.done[idx]\n",
    "        return state, action, next_states, pos, reward, done, num_agents\n",
    "\n",
    "def get_smol_dataset(states, actions, next_states, pos, num_agents, rewards, done, num_samples:int=None):\n",
    "    if num_samples is None:\n",
    "        return ImitationDataset(states, actions, next_states, pos, num_agents, rewards, done)\n",
    "        \n",
    "    chosen_indices = np.random.choice(np.arange(len(states)), num_samples, replace=False)\n",
    "    final_indices = np.array(chosen_indices)\n",
    "\n",
    "    smol_states = states[final_indices]\n",
    "    smol_actions = actions[final_indices]\n",
    "    smol_next_states = next_states[final_indices]\n",
    "    smol_pos = pos[final_indices]\n",
    "    smol_num_agents = num_agents[final_indices]\n",
    "    smol_rewards = rewards[final_indices]\n",
    "    smol_done = done[final_indices]\n",
    "    return ImitationDataset(smol_states, smol_actions, smol_next_states, smol_pos, smol_num_agents, smol_rewards, smol_done)\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    val_size = int(val_ratio * dataset_size)\n",
    "    # test size is the remainder\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size+val_size]\n",
    "    test_indices = indices[train_size+val_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def get_dataset_and_dataloaders(\n",
    "    train_bs:int=128,\n",
    "    val_bs:int=128,\n",
    "    test_bs:int=1,\n",
    "    actor_num_samples=1000,\n",
    "    critic_num_samples=10000,\n",
    "    rb_path='../../data/replay_buffer_mixed_obj.pkl'\n",
    "):\n",
    "    replay_buffer = pkl.load(open(rb_path, 'rb'))\n",
    "\n",
    "    obs = replay_buffer['obs']\n",
    "    act = replay_buffer['act']\n",
    "    obs2 = replay_buffer['obs2']\n",
    "    pos = replay_buffer['pos']\n",
    "    num_agents = replay_buffer['num_agents']\n",
    "    rewards = replay_buffer['rew']\n",
    "    done = replay_buffer['done']\n",
    "\n",
    "    # Actor: only actions with rewards > 30\n",
    "    actor_idxs = np.where(rewards > 30)[0]\n",
    "    actor_dataset = get_smol_dataset(\n",
    "        obs[actor_idxs], act[actor_idxs], obs2[actor_idxs],\n",
    "        pos[actor_idxs], num_agents[actor_idxs],\n",
    "        rewards[actor_idxs], done[actor_idxs],\n",
    "        num_samples=actor_num_samples\n",
    "    )\n",
    "\n",
    "    # Split actor dataset\n",
    "    actor_train_dataset, actor_val_dataset, actor_test_dataset = split_dataset(actor_dataset)\n",
    "    actor_train_loader = DataLoader(actor_train_dataset, batch_size=train_bs, shuffle=True)\n",
    "    actor_val_loader = DataLoader(actor_val_dataset, batch_size=val_bs, shuffle=False)\n",
    "    actor_test_loader = DataLoader(actor_test_dataset, batch_size=test_bs, shuffle=False)\n",
    "\n",
    "    # Critic: Combine good and bad experiences into one dataset\n",
    "    critic_good_idxs = np.where(rewards > 70)[0]\n",
    "    critic_bad_idxs = np.where((rewards <= 30) & (rewards > -20) & (rewards != 0))[0]\n",
    "    critic_all_idxs = np.concatenate([critic_good_idxs, critic_bad_idxs])\n",
    "\n",
    "    critic_dataset = get_smol_dataset(\n",
    "        obs[critic_all_idxs], act[critic_all_idxs], obs2[critic_all_idxs],\n",
    "        pos[critic_all_idxs], num_agents[critic_all_idxs],\n",
    "        rewards[critic_all_idxs], done[critic_all_idxs],\n",
    "        num_samples=critic_num_samples\n",
    "    )\n",
    "\n",
    "    critic_train_loader = DataLoader(critic_dataset, batch_size=train_bs, shuffle=True)\n",
    "\n",
    "    return actor_train_loader, actor_val_loader, actor_test_loader, critic_train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2000\n",
    "train_bs = 256\n",
    "val_bs = 256\n",
    "test_bs = 1\n",
    "actor_train_loader, actor_val_loader, actor_test_loader, critic_train_loader = get_dataset_and_dataloaders(\n",
    "    train_bs=train_bs,\n",
    "    val_bs=val_bs,\n",
    "    test_bs=test_bs,\n",
    "    actor_num_samples=num_samples,\n",
    "    critic_num_samples=num_samples*5,\n",
    "    rb_path='../../data/replay_buffer_mixed_obj.pkl'\n",
    ")\n",
    "len(actor_train_loader), len(actor_val_loader), len(actor_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fd98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "hp_dict = {\n",
    "        \"exp_name\"          : \"MATBC\",\n",
    "        \"data_dir\"          : \"./data/rl_data\",\n",
    "        'warmup_iters'      : 2,\n",
    "        'pi_lr'             : 1e-4,\n",
    "        'pi_eta_min'        : 1e-6,\n",
    "        'q_lr'              : 1e-6,\n",
    "        'q_eta_min'         : 1e-8,\n",
    "        'epochs'            : n_epochs,\n",
    "        'sce'               : \"./sce.pth\",\n",
    "\n",
    "        # DiT Params:\n",
    "        'state_dim'         : 6,\n",
    "        'action_dim'        : 3,\n",
    "        'act_limit'         : 0.03,\n",
    "        'gamma'             : 0.99,\n",
    "        \"dev_rl\"            : torch.device(\"cuda:0\"),\n",
    "        \"model_dim\"         : 256,\n",
    "        \"num_heads\"         : 8,\n",
    "        \"dim_ff\"            : 512,\n",
    "        \"n_layers_dict\"     : {'encoder':5, 'actor': 10, 'critic': 10},\n",
    "        \"dropout\"           : 0,\n",
    "        \"max_grad_norm\"     : 1,\n",
    "        'gauss'             : True,\n",
    "        'masked'            : True,\n",
    "        'adaln'             : True,\n",
    "        'attn_mech'         : 'AdaLN',\n",
    "    }\n",
    "\n",
    "model = Transformer(hp_dict)\n",
    "model.to(hp_dict['dev_rl'])\n",
    "optimizer_actor = optim.AdamW(model.decoder_actor.parameters(), lr=hp_dict['pi_lr'], weight_decay=1e-2)\n",
    "optimizer_critic = optim.AdamW(model.decoder_critic.parameters(), lr=hp_dict['q_lr'], weight_decay=1e-2)\n",
    "\n",
    "lr_scheduler_actor = CosineAnnealingWarmRestarts(optimizer_actor, T_0=20, T_mult=2, eta_min=hp_dict['pi_eta_min'])\n",
    "lr_scheduler_critic = CosineAnnealingWarmRestarts(optimizer_critic, T_0=20, T_mult=2, eta_min=hp_dict['q_eta_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_losses = []\n",
    "q_losses = []\n",
    "val_pi_losses = []\n",
    "val_q_losses = []\n",
    "global_Step = 0\n",
    "start_value = 1\n",
    "end_value = 1e2\n",
    "steps = hp_dict['warmup_iters']\n",
    "\n",
    "increment = (end_value - start_value) / (steps - 1)\n",
    "values = [start_value + i * increment for i in range(steps)]\n",
    "\n",
    "# Set initial learning rates\n",
    "for param_group in optimizer_actor.param_groups:\n",
    "    param_group['lr'] = hp_dict['pi_eta_min']\n",
    "for param_group in optimizer_critic.param_groups:\n",
    "    param_group['lr'] = hp_dict['q_eta_min']\n",
    "\n",
    "# Main training loop for both actor and critic\n",
    "for epoch in range(hp_dict['epochs']):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(actor_train_loader, desc=f\"Training Epoch {epoch}\", leave=True, mininterval=1) as t:\n",
    "        for i, (s1, a, s2, p, r, d, N) in enumerate(t):\n",
    "            N = int(torch.max(N))\n",
    "            \n",
    "            # Move tensors to correct device\n",
    "            actions = a[:, :N].to(hp_dict['dev_rl'])\n",
    "            bs = actions.shape[0]\n",
    "            ones_column = -0.02*torch.rand(bs, N, 1, device=hp_dict['dev_rl']) + 0.01\n",
    "            actions_3d = torch.cat([actions, ones_column], dim=2)\n",
    "            states = s1[:, :N].to(hp_dict['dev_rl'])\n",
    "            next_states = s2[:, :N].to(hp_dict['dev_rl'])\n",
    "            pos = p[:, :N].to(hp_dict['dev_rl'])\n",
    "            rewards = r.to(hp_dict['dev_rl'])\n",
    "            dones = d.to(hp_dict['dev_rl'])\n",
    "\n",
    "            # Actor update\n",
    "            optimizer_actor.zero_grad()\n",
    "            pi_loss = model.compute_actor_loss(actions_3d, states, pos)\n",
    "            pi_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer_actor.step()\n",
    "            \n",
    "            # Critic update\n",
    "            optimizer_critic.zero_grad()\n",
    "            q_loss = model.compute_critic_loss(states, actions_3d, next_states, pos, rewards, dones)\n",
    "            q_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "            # Update learning rates if using a warmup or scheduler\n",
    "            if global_Step >= (steps - 1):\n",
    "                lr_scheduler_actor.step()\n",
    "                lr_scheduler_critic.step()\n",
    "            else:\n",
    "                for param_group in optimizer_actor.param_groups:\n",
    "                    param_group['lr'] = hp_dict['pi_eta_min'] * values[global_Step]\n",
    "                for param_group in optimizer_critic.param_groups:\n",
    "                    param_group['lr'] = hp_dict['q_eta_min'] * values[global_Step]\n",
    "            \n",
    "            pi_losses.append(pi_loss.item())\n",
    "            q_losses.append(q_loss.item())\n",
    "            t.set_postfix({\n",
    "                \"pi_loss\": np.mean(pi_losses[-len(actor_train_loader):]), \n",
    "                \"q_loss\": np.mean(q_losses[-len(actor_train_loader):])\n",
    "            }, refresh=False)\n",
    "            \n",
    "            global_Step += 1\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % (hp_dict['epochs']//10) == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm.tqdm(actor_val_loader, desc=f\"Validation Epoch {epoch}\", leave=True, mininterval=1) as v:\n",
    "                for i, (s1, a, s2, p, r, d, N) in enumerate(v):\n",
    "                    N = int(torch.max(N))\n",
    "                    actions = a[:, :N].to(hp_dict['dev_rl'])\n",
    "                    bs = actions.shape[0]\n",
    "                    ones_column = -0.02*torch.rand(bs, N, 1, device=hp_dict['dev_rl']) + 0.01\n",
    "                    actions_3d = torch.cat([actions, ones_column], dim=2)\n",
    "                    states = s1[:, :N].to(hp_dict['dev_rl'])\n",
    "                    next_states = s2[:, :N].to(hp_dict['dev_rl'])\n",
    "                    pos = p[:, :N].to(hp_dict['dev_rl'])\n",
    "                    rewards = r.to(hp_dict['dev_rl'])\n",
    "                    dones = d.to(hp_dict['dev_rl'])\n",
    "                    \n",
    "                    val_pi_loss = 1000 * model.compute_actor_loss(actions_3d, states, pos).item()\n",
    "                    val_q_loss = model.compute_critic_loss(states, actions_3d, next_states, pos, rewards, dones).item()\n",
    "                    \n",
    "                    val_pi_losses.append(val_pi_loss)\n",
    "                    val_q_losses.append(val_q_loss)\n",
    "\n",
    "                    v.set_postfix({\n",
    "                        \"val_pi_loss\": np.mean(val_pi_losses[-len(actor_train_loader):]), \n",
    "                        \"val_q_loss\": np.mean(val_q_losses[-len(actor_train_loader):])\n",
    "                    }, refresh=False)\n",
    "        model.train()\n",
    "\n",
    "##############################################\n",
    "# After finishing the main loop for actor training\n",
    "# Now run a separate training loop for the critic only\n",
    "##############################################\n",
    "\n",
    "critic_only_epochs = hp_dict.get('critic_only_epochs', 10)  # Number of epochs only training the critic\n",
    "critic_only_q_losses = []\n",
    "\n",
    "for critic_epoch in range(critic_only_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(critic_train_loader, desc=f\"Critic Only Training Epoch {critic_epoch}\", leave=True, mininterval=1) as ct:\n",
    "        for i, (s1, a, s2, p, r, d, N) in enumerate(ct):\n",
    "            N = int(torch.max(N))\n",
    "\n",
    "            # Move tensors to correct device\n",
    "            actions = a[:, :N].to(hp_dict['dev_rl'])\n",
    "            bs = actions.shape[0]\n",
    "            ones_column = -0.02*torch.rand(bs, N, 1, device=hp_dict['dev_rl']) + 0.01\n",
    "            actions_3d = torch.cat([actions, ones_column], dim=2)\n",
    "            states = s1[:, :N].to(hp_dict['dev_rl'])\n",
    "            next_states = s2[:, :N].to(hp_dict['dev_rl'])\n",
    "            pos = p[:, :N].to(hp_dict['dev_rl'])\n",
    "            rewards = r.to(hp_dict['dev_rl'])\n",
    "            dones = d.to(hp_dict['dev_rl'])\n",
    "\n",
    "            # Only update the critic\n",
    "            optimizer_critic.zero_grad()\n",
    "            q_loss = model.compute_critic_loss(states, actions_3d, next_states, pos, rewards, dones)\n",
    "            q_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "            critic_only_q_losses.append(q_loss.item())\n",
    "            ct.set_postfix({\"critic_q_loss\": np.mean(critic_only_q_losses[-len(critic_train_loader):])}, refresh=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_dict = {\"model\": model.state_dict(), \"actor_optimizer\": optimizer_actor.state_dict(), \"critic_optimizer\": optimizer_critic.state_dict()}\n",
    "torch.save(expt_dict, '../../pretrained_ckpts/matbc_pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76802d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Uncomment to load a pretrained model for visualization \"\"\"\n",
    "# model.load_state_dict(torch.load('../../models/trained_models/matbc_finetuned.pt', weights_only=False, map_location=hp_dict['dev_rl'])['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_bkp = pi_losses\n",
    "val_losses_bkp = val_pi_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0338c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses_bkp[0:])\n",
    "val_losses = np.array(val_losses_bkp[0:])\n",
    "x_val = np.linspace(0, len(train_losses) - 1, len(val_losses))\n",
    "x_train = np.arange(len(train_losses))\n",
    "val_losses_interpolated = np.interp(x_train, x_val, val_losses)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_losses_normalized = scaler.fit_transform(train_losses.reshape(-1, 1)).flatten()\n",
    "val_losses_normalized = scaler.fit_transform(val_losses_interpolated.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses_normalized, label='Train Loss (Normalized)')\n",
    "plt.plot(val_losses_normalized, label='Validation Loss (Normalized)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Normalized Loss')\n",
    "plt.title('Normalized Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "\n",
    "model.eval()\n",
    "loss = []\n",
    "statesss = []\n",
    "actionsss = []\n",
    "actions_gt = []\n",
    "possss = []\n",
    "\n",
    "grand_data = np.zeros((100, 128, 10, 2))\n",
    "with torch.no_grad():\n",
    "    with tqdm.tqdm(actor_test_loader, desc=\"Validation\", leave=False, mininterval=1) as v:\n",
    "        for i, (s1, a, s2, p, r, d, N) in enumerate(v):\n",
    "            N = int(torch.max(N))\n",
    "            \n",
    "            actions = a[:1, :N].to(hp_dict['dev_rl'])\n",
    "            bs = actions.shape[0]\n",
    "            ones_column = -0.02*torch.rand(bs, N, 1, device=hp_dict['dev_rl']) + 0.01\n",
    "            actions_3d = torch.cat([actions, ones_column], dim=2)\n",
    "            actions_gt.append(actions)\n",
    "            states = s1[:1, :N].to(hp_dict['dev_rl'])\n",
    "            statesss.append(states)\n",
    "            next_states = s2[:, :N].to(hp_dict['dev_rl'])\n",
    "            pos = p[:1, :N].to(hp_dict['dev_rl'])\n",
    "            possss.append(pos)\n",
    "    \n",
    "            output_actions = model.get_actions(states, pos, deterministic=True)[:,:N,:2]\n",
    "            \n",
    "            actionsss.append(output_actions)\n",
    "            loss.append((actions[:1, :N].cpu().detach().numpy() - output_actions.cpu().detach().numpy()).mean()**2)\n",
    "            \n",
    "            if i>60:\n",
    "                break\n",
    "plt.plot(loss)\n",
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for n, (state, action, action_gt, pos) in enumerate(zip(statesss, actionsss, actions_gt, possss)):\n",
    "    state, action, action_gt, pos = state, action, action_gt, pos\n",
    "    pos = pos.detach().cpu().numpy()\n",
    "    acts = action.detach().cpu().numpy()\n",
    "    acts_gt = action_gt.detach().cpu().numpy()\n",
    "    state = state.detach().cpu().numpy()\n",
    "\n",
    "    for n, idx in enumerate(range(state.shape[0])):\n",
    "        po = pos[idx]\n",
    "        n_agents = len([p for p in po if p != 0])\n",
    "        r_poses = kdtree_positions_world[po[:n_agents]]\n",
    "        init_pts = state[idx][:n_agents,:2] + r_poses\n",
    "        goal_bd_pts = state[idx][:n_agents,2:4] + r_poses\n",
    "        act_grsp = state[idx][:n_agents,4:6]\n",
    "        act = acts[idx]\n",
    "        act_gt = acts_gt[idx, :n_agents]\n",
    "        r_poses2 = r_poses + act_grsp\n",
    "\n",
    "        losses.append(np.linalg.norm(act - act_gt))\n",
    "\n",
    "        print(n_agents)\n",
    "        plt.figure(figsize=(10,17.78))\n",
    "        plt.scatter(kdtree_positions_world[:, 0], kdtree_positions_world[:, 1], c='#ddddddff')\n",
    "        plt.scatter(init_pts[:, 0], init_pts[:, 1], c = '#00ff00ff')\n",
    "        plt.scatter(goal_bd_pts[:, 0], goal_bd_pts[:, 1], c='red')\n",
    "\n",
    "        plt.quiver(r_poses2[:, 0], r_poses2[:, 1], act[:, 0], act[:, 1], color='#ff0000aa')\n",
    "        plt.quiver(r_poses2[:, 0], r_poses2[:, 1], act_gt[:, 0], act_gt[:, 1], color='#aaff55aa')\n",
    "\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.show()\n",
    "        if n%30 == 0:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
